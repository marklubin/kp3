"""initial schema

Revision ID: e935a08a6245
Revises:
Create Date: 2025-12-13 16:53:17.612828

"""

from typing import Sequence, Union

import pgvector.sqlalchemy
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

from alembic import op

# revision identifiers, used by Alembic.
revision: str = "e935a08a6245"
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # Enable pgvector extension
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")

    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "passages",
        sa.Column("id", sa.UUID(), server_default=sa.text("gen_random_uuid()"), nullable=False),
        sa.Column("content", sa.Text(), nullable=False),
        sa.Column("content_hash", sa.String(length=64), nullable=False),
        sa.Column("content_tsv", postgresql.TSVECTOR(), nullable=True),
        sa.Column("passage_type", sa.String(length=64), nullable=False),
        sa.Column("period_start", sa.DateTime(timezone=True), nullable=True),
        sa.Column("period_end", sa.DateTime(timezone=True), nullable=True),
        sa.Column(
            "metadata", postgresql.JSONB(astext_type=sa.Text()), server_default="{}", nullable=False
        ),
        sa.Column("source_system", sa.String(length=64), nullable=True),
        sa.Column("source_external_id", sa.String(length=256), nullable=True),
        sa.Column("embedding_qwen3", pgvector.sqlalchemy.vector.VECTOR(dim=1024), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("content_hash"),
    )
    op.create_index(
        "idx_passages_embedding",
        "passages",
        ["embedding_qwen3"],
        unique=False,
        postgresql_using="ivfflat",
        postgresql_ops={"embedding_qwen3": "vector_cosine_ops"},
    )
    op.create_index("idx_passages_period", "passages", ["period_start", "period_end"], unique=False)
    op.create_index("idx_passages_type", "passages", ["passage_type"], unique=False)

    # Replace content_tsv with a generated column and recreate index
    op.execute("ALTER TABLE passages DROP COLUMN content_tsv")
    op.execute("""
        ALTER TABLE passages ADD COLUMN content_tsv tsvector
        GENERATED ALWAYS AS (to_tsvector('english', content)) STORED
    """)
    op.execute("CREATE INDEX idx_passages_tsv ON passages USING gin(content_tsv)")
    op.create_table(
        "processing_runs",
        sa.Column("id", sa.UUID(), server_default=sa.text("gen_random_uuid()"), nullable=False),
        sa.Column("input_sql", sa.Text(), nullable=False),
        sa.Column("processor_type", sa.String(length=64), nullable=False),
        sa.Column("processor_config", postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column("status", sa.String(length=32), nullable=False),
        sa.Column("total_groups", sa.Integer(), nullable=True),
        sa.Column("processed_groups", sa.Integer(), nullable=False),
        sa.Column("output_count", sa.Integer(), nullable=False),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column("started_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("completed_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index("idx_runs_status", "processing_runs", ["status"], unique=False)
    op.create_table(
        "passage_derivations",
        sa.Column("id", sa.UUID(), server_default=sa.text("gen_random_uuid()"), nullable=False),
        sa.Column("derived_passage_id", sa.UUID(), nullable=False),
        sa.Column("source_passage_id", sa.UUID(), nullable=False),
        sa.Column("processing_run_id", sa.UUID(), nullable=False),
        sa.Column("source_order", sa.Integer(), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["derived_passage_id"],
            ["passages.id"],
        ),
        sa.ForeignKeyConstraint(
            ["processing_run_id"],
            ["processing_runs.id"],
        ),
        sa.ForeignKeyConstraint(
            ["source_passage_id"],
            ["passages.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("derived_passage_id", "source_passage_id"),
    )
    op.create_index(
        "idx_derivations_derived", "passage_derivations", ["derived_passage_id"], unique=False
    )
    op.create_index(
        "idx_derivations_run", "passage_derivations", ["processing_run_id"], unique=False
    )
    op.create_index(
        "idx_derivations_source", "passage_derivations", ["source_passage_id"], unique=False
    )
    op.create_table(
        "passages_archive",
        sa.Column(
            "archive_id", sa.UUID(), server_default=sa.text("gen_random_uuid()"), nullable=False
        ),
        sa.Column(
            "archived_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column("archived_by_run_id", sa.UUID(), nullable=True),
        sa.Column("id", sa.UUID(), nullable=False),
        sa.Column("content", sa.Text(), nullable=False),
        sa.Column("content_hash", sa.String(length=64), nullable=False),
        sa.Column("passage_type", sa.String(length=64), nullable=False),
        sa.Column("period_start", sa.DateTime(timezone=True), nullable=True),
        sa.Column("period_end", sa.DateTime(timezone=True), nullable=True),
        sa.Column("metadata", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("source_system", sa.String(length=64), nullable=True),
        sa.Column("source_external_id", sa.String(length=256), nullable=True),
        sa.Column("embedding_qwen3", pgvector.sqlalchemy.vector.VECTOR(dim=1024), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(
            ["archived_by_run_id"],
            ["processing_runs.id"],
        ),
        sa.PrimaryKeyConstraint("archive_id"),
    )
    op.create_index("idx_archive_passage", "passages_archive", ["id"], unique=False)
    op.create_index("idx_archive_run", "passages_archive", ["archived_by_run_id"], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index("idx_archive_run", table_name="passages_archive")
    op.drop_index("idx_archive_passage", table_name="passages_archive")
    op.drop_table("passages_archive")
    op.drop_index("idx_derivations_source", table_name="passage_derivations")
    op.drop_index("idx_derivations_run", table_name="passage_derivations")
    op.drop_index("idx_derivations_derived", table_name="passage_derivations")
    op.drop_table("passage_derivations")
    op.drop_index("idx_runs_status", table_name="processing_runs")
    op.drop_table("processing_runs")
    op.drop_index("idx_passages_type", table_name="passages")
    op.drop_index("idx_passages_tsv", table_name="passages", postgresql_using="gin")
    op.drop_index("idx_passages_period", table_name="passages")
    op.drop_index(
        "idx_passages_embedding",
        table_name="passages",
        postgresql_using="ivfflat",
        postgresql_ops={"embedding_qwen3": "vector_cosine_ops"},
    )
    op.drop_table("passages")
    # ### end Alembic commands ###
